{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOfU1wHPBdJhrGj3M8PVA7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyjhub/OSS_project/blob/main/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4_%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리"
      ],
      "metadata": {
        "id": "zZYQEoaJ7sfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from urllib.parse import quote\n",
        "import requests\n",
        "import pandas as pd\n",
        "from urllib.parse import quote\n",
        "from urllib.error import HTTPError, URLError\n",
        "import cv2 \n",
        "from google.colab.patches import cv2_imshow\n",
        "import re\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "Xqx2uWh_6uDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 영화제목 입력"
      ],
      "metadata": {
        "id": "2_EJyCSFs6tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_name_ = input('어떤 영화를 검색하시겠습니까 : ')\n",
        "movie_name = quote(movie_name_)"
      ],
      "metadata": {
        "id": "83bBpBmF6VdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://movie.naver.com/movie/search/result.naver?query=' + movie_name + '&section=all&ie=utf8'"
      ],
      "metadata": {
        "id": "J5nVM-gv6hec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 입력값에 대한 영화 목록\n",
        "- 영화 제목을 완벽하게 알지 못할 때 기억나는 단어만 입력하면  \n",
        "영화 제목에 입력 문자열이 있는 영화 정보와 포스터 출력"
      ],
      "metadata": {
        "id": "FStsSHa-CpT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_page = requests.get(url).content\n",
        "movie_page_soup = BeautifulSoup(movie_page, 'html.parser')\n",
        "#더 많은 영화보기 버튼 url 추출\n",
        "more_exist = False\n",
        "more_button = movie_page_soup.find('div', {'class' : 'search_all'}).find('a', {'class' : 'more_list'})\n",
        "if more_button.get_text() == '더 많은 영화 보기':\n",
        "  more_exist = True\n",
        "  url = 'http://movie.naver.com/movie/search/result.naver?section=movie&query=' + movie_name\n",
        "  #more_url = re.findall(r'\\/[\\w\\W]+\\\"', str(more_button))\n",
        "  #more_url = more_url[0][:-1]"
      ],
      "metadata": {
        "id": "_gkNlUsBf7ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num=1\n",
        "movie_codes = []\n",
        "while True:\n",
        "  movie_page = requests.get(url).content\n",
        "  movie_page_soup = BeautifulSoup(movie_page, 'html.parser')\n",
        "  movie_lists = movie_page_soup.find('ul', {'class' : 'search_list_1'}).find_all('li')\n",
        "  for movie in movie_lists:\n",
        "    try:\n",
        "      movie_img_url = movie.find('img').get('src')\n",
        "      urllib.request.urlretrieve(movie_img_url, './sample_data/' + str(num) + '.jpg')\n",
        "      img = cv2.imread('./sample_data/' + str(num) + '.jpg')\n",
        "      href = movie.find('a').get('href')\n",
        "      movie_code = href.split('=')[1]\n",
        "      movie_codes.append(movie_code)\n",
        "      print('===================' + str(num) + '번째 영화==================')\n",
        "      print(movie.dl.get_text()[1:-3].strip())\n",
        "      cv2_imshow(img)\n",
        "      print()\n",
        "      num+=1\n",
        "    except AttributeError as e:\n",
        "      print('영화 표지가 없습니다.')\n",
        "  try:\n",
        "    url = 'http://movie.naver.com' + movie_page_soup.find('td', {'class' : 'next'}).find('a').get('href')\n",
        "  except:\n",
        "    break"
      ],
      "metadata": {
        "id": "hb16pAmL6_fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(input('몇번째 영화가 찾던 영화입니까? '))\n",
        "search_movie_code = movie_codes[n-1]\n",
        "print(search_movie_code)"
      ],
      "metadata": {
        "id": "hKagYTThKohp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전체평점 다 볼 수 있는 페이지 스크래핑\n",
        "\n",
        "- 2167 리뷰에서 HTTPERROR 413 떠서 try except 넣어줌  \n",
        "- 13번째 코드 url을 for문 밖에다가 뒀더니 16번째 코드 실행할 때 url1 -> url12 -> url123이렇게 떠서 범위를 벗어나서 http error 떴던것임 13번째코드를 지금처럼 for문 안에 넣었더니 해결됨"
      ],
      "metadata": {
        "id": "C-6qohedsEWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "movie_review_df = pd.DataFrame(columns=(\"Score\", \"Review\"))\n",
        "\n",
        "def get_movie_review(movie_code):\n",
        "  num = 0\n",
        "  count = 0\n",
        "  url = 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.naver?code=' + str(movie_code) + '&type=after&isActualPointWriteExecute=false&isMileageSubscriptionAlready=false&isMileageSubscriptionReject=false&page=1'  \n",
        "    \n",
        "  while True:\n",
        "    try:\n",
        "      print(\"페이지\" + str(num) + \" 리뷰\")\n",
        "      \n",
        "      review_page = urllib.request.urlopen(url).read()\n",
        "      review_page_soup = BeautifulSoup(review_page, 'html.parser')\n",
        "      review_list = review_page_soup.select('body > div > div > div.score_result > ul > li')  #> li\n",
        "      for idx, review_info in enumerate(review_list):\n",
        "        score = review_info.find('em').get_text()\n",
        "        review_text = review_info.find('span', {'id':'_filtered_ment_{}'.format(idx)}).get_text().strip()\n",
        "        movie_review_df.loc[count] = [score, review_text]\n",
        "        count += 1\n",
        "      try:\n",
        "        url = 'http://movie.naver.com' + review_page_soup.find('a', {'class' : 'pg_next'}).get('href')\n",
        "        num += 1\n",
        "      except:\n",
        "        break\n",
        "\n",
        "    except HTTPError as e:\n",
        "      print(e)\n",
        "      pass\n",
        "  \n",
        "  return movie_review_df"
      ],
      "metadata": {
        "id": "evC3uriqZq6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 리뷰 데이터 확인"
      ],
      "metadata": {
        "id": "MawXlE5sDlub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = get_movie_review(search_movie_code)\n",
        "review"
      ],
      "metadata": {
        "id": "wHrbndvduBY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터프레임 csv파일로 저장\n",
        "\n",
        "- encoding 안 정해줬더니 파일이 깨져서 encoding 설정"
      ],
      "metadata": {
        "id": "cYa0FY-OoyU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review.to_csv('./review.csv', encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "C6LzNxaVC5xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결측치 제거와 리뷰 데이터 개요 출력"
      ],
      "metadata": {
        "id": "AWnnpV53MgBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#리뷰데이터 불러오기\n",
        "#review = pd.read_csv('./sample_data/interstella_review.csv')\n",
        "#결측치 제거 전\n",
        "before = review.shape[0]\n",
        "\n",
        "review.dropna(inplace=True)\n",
        "\n",
        "#결측치 제거 후\n",
        "after = review.shape[0]\n",
        "\n",
        "drop_n = before - after\n",
        "print('결측치 제거 행 개수 = ' + str(drop_n))\n",
        "\n",
        "print('반올림된 리뷰 평점 평균 : ' + str(review['Score'].mean().round()))\n",
        "\n",
        "# 불용어 제거할 때 쉽게 하기위해 리스트로 변환\n",
        "review_dict = review.to_dict()\n",
        "review_val = review_dict['Review'].values()\n",
        "reviews = list(review_val)\n",
        "\n",
        "print('전체 리뷰 수 : ' + str(len(reviews)))"
      ],
      "metadata": {
        "id": "VanwHqmy2lyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한국어 형태소 분석기 설치\n",
        "- MeCab 설치"
      ],
      "metadata": {
        "id": "uA9U5-Ss7pVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ],
      "metadata": {
        "id": "gzdJ9vqc7nh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "tagger = Mecab()"
      ],
      "metadata": {
        "id": "YKc603mH7vKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 영화리뷰에 관련 없는 용어 제거"
      ],
      "metadata": {
        "id": "vYVEhkxG7Id8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "garbage = '영화 전 난 일 걸 뭐 줄 만 건 분 개 끝 잼 이거 번 중 듯 때 게 내 말 나 수 거 점 것 후 이 애 씨 속 뿐 밋 그 급 ㄷ 데'\n",
        "garbage = garbage.split(' ')"
      ],
      "metadata": {
        "id": "PxWPl5Q61YxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_nouns = []\n",
        "\n",
        "for review in reviews:\n",
        "  for noun in tagger.nouns(review):\n",
        "    if noun not in garbage:\n",
        "      review_nouns.append(noun)\n",
        "\n",
        "review_nouns[:10]"
      ],
      "metadata": {
        "id": "RGn1dEnN1syn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 빈도수"
      ],
      "metadata": {
        "id": "wq9-1VfqXwUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "review_noun_count = Counter(review_nouns)\n",
        "top_review_nouns = dict(review_noun_count.most_common(100))\n",
        "top_review_nouns"
      ],
      "metadata": {
        "id": "fFb4IQoV8OYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 한글 폰트 설정\n",
        "- 그래프 폰트 깨짐 수정하기 위해\n",
        "- 출처 : https://didalsgur.tistory.com/entry/Python-Colab-%EC%82%AC%EC%9A%A9-%EC%8B%9C-%ED%95%9C%EA%B8%80-%EA%B9%A8%EC%A7%90-%ED%98%84%EC%83%81-%ED%95%B4%EA%B2%B0-feat-matplotlib"
      ],
      "metadata": {
        "id": "2bdt2ObHaI_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_font():\n",
        "    # From https://HC.Dle.pw, By Jinseo Kim\n",
        "    # v1.0.0\n",
        "    import os\n",
        "    import matplotlib as mpl\n",
        "    import matplotlib.pyplot as plt\n",
        "    os.system(\"apt-get install -y fonts-nanum\")\n",
        "    os.system(\"fc-cache -fv\")\n",
        "    mpl.font_manager._rebuild()\n",
        "    findfont = mpl.font_manager.fontManager.findfont\n",
        "    mpl.font_manager.findfont = findfont\n",
        "    mpl.backends.backend_agg.findfont = findfont\n",
        "    plt.rcParams['font.family'] = \"NanumBarunGothic\"\n",
        "    plt.rcParams['axes.unicode_minus'] = False\n",
        "           \n",
        "fix_font()"
      ],
      "metadata": {
        "id": "LRdkoXWDRyzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import font_manager, rc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pos = np.arange(len(top_review_nouns))\n",
        "\n",
        "plt.figure(figsize=(12, 24))\n",
        "plt.barh(y_pos, top_review_nouns.values()) #수평 바차트\n",
        "plt.title('Word Count')\n",
        "plt.yticks(y_pos, top_review_nouns.keys())\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "plt.show();"
      ],
      "metadata": {
        "id": "vhQLfAyhXh0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "OVKDkTOnY-Ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(background_color='white', font_path='./font/NanumBarunGothic.ttf')\n",
        "wc.generate_from_frequencies(top_review_nouns)"
      ],
      "metadata": {
        "id": "DeQDSPSQkk8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "figure = plt.figure(figsize=(12, 12))\n",
        "ax = figure.add_subplot(1, 1, 1)\n",
        "ax.axis('off')\n",
        "ax.imshow(wc)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u-uvsvYlknpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다대일 \n",
        "  - 입력이 여러개고 출력은 하나\n",
        "  - 감정분석에 사용됨"
      ],
      "metadata": {
        "id": "HTBPLPk3PSzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 언어를 숫자로 표현하는 것이 embedding"
      ],
      "metadata": {
        "id": "SFfMbpDU_xXr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmJDckweJu_V"
      },
      "source": [
        "# 환경 준비\n",
        "1. 라이브러리 다운로드\n",
        "2. 네이버 영화평과 긍부정 데이터를 다운로드합니다 (파일보기 +  새로고침 후 확인)\n",
        "\n",
        "* 자체 데이터셋을 사용할 경우 내용과 카테고리가 각각 content와 label 열에 들어가는 파일(아래 예시 참조)로 dataset.xlsx로 저장 후 기존 파일을 덮어쓰기 하면 됩니다. \n",
        "* 엑셀파일의 label과 content의 순서는 상관없으나 label은 0부터 시작하는 숫자로 입력하면 좋습니다. 예를들어 카테고리가 4개면 label을 0, 1, 2, 3으로 표시해주세요.\n",
        "\n",
        "```\n",
        "label  content\n",
        "1      영화가 재밌다.     \n",
        "1      이 영화 추천해요.     \n",
        "0      지루한 영화였습니다.\n",
        "...  \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceUDoT3ZEfWB"
      },
      "source": [
        "!pip3 install -q transformers\n",
        "!git clone https://github.com/kiyoungkim1/ReadyToUseAI\n",
        "\n",
        "from ReadyToUseAI.src.nlp import make_sample_dataset, bert_sequence_classification\n",
        "make_sample_dataset.nsmc(mode='test', text_only=False)  # mode: which datasets? 'train' or 'test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2150z24Na2-"
      },
      "source": [
        "# [Training] \n",
        "* 첨부된 샘플의 경우 약 40min 소요 (Tesla T4 GPU)\n",
        "* min_sentence_length보다 긴 문장만 사용합니다.\n",
        "* MAX_LEN은 모델이 인식하는 token의 길이로, 전체길이가 약 MAX_LEN의 2배보다 긴 문장은 뒷부분이 삭제됩니다 (예를들어 MAX_LEN = 128이면, 대략 길이가 256이상인 문장은 뒷부분이 무시됨).\n",
        "* batch_size는 한번에 몇개의 sample을 계산하는지를 나타내며, 제한된 메모리에서 MAX_LEN을 줄이면 batch_size를 키울 수 있고, MAX_LEN를 키우면 batch_size를 줄여야 합니다. \n",
        "* epochs는 데이터셋을 몇번 반복해서 학습할지 여부이며, dataset_split은 전체 데이터 중 몇 %를 검증용 데이터셋으로 사용할지 여부입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNkyff8jK9Av"
      },
      "source": [
        "CLS = bert_sequence_classification.Classification(model_name='kykim/bert-kor-base', min_sentence_length=10, MAX_LEN=128, batch_size=32, use_bert_tokenizer=True)\n",
        "CLS.dataset(data_path='dataset.xlsx')\n",
        "CLS.load_model(mode='train')\n",
        "CLS.train(epochs=3, dataset_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiiPTYtOmcF"
      },
      "source": [
        "# [Inference]\n",
        "* sentences에 원하는 문장을 아래 형식과 같이 넣으면 해당하는 카테고리를 반환합니다.\n",
        "* saved_model_path는 학습된 모델이 저장된 '폴더명'입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jZ5DtJmNLr-"
      },
      "source": [
        "#sentences = ['영화 재밌어요', '영화 재미없어요', '그냥 시간떼우기용', '완전 추천작']\n",
        "saved_model_path='model/saved/3'\n",
        "\n",
        "CLS = bert_sequence_classification.Classification(model_name='kykim/bert-kor-base', min_sentence_length=10, MAX_LEN=128, batch_size=64, use_bert_tokenizer=True)\n",
        "CLS.load_model(mode='inference', saved_model_path=saved_model_path)\n",
        "# reviews = 크롤링한 리뷰 데이터\n",
        "review_down = pd.read_csv('./review.csv', encoding='utf-8')\n",
        "review_frame = review_down['Review']\n",
        "before = len(review_frame)\n",
        "review_frame.dropna(inplace=True)\n",
        "after = len(review_frame)\n",
        "review_list = review_frame.values.tolist()\n",
        "print('컴퓨터에 저장된 리뷰 다운받으면서 손실된 리뷰 개수 : ', before - after)\n",
        "print('총 리뷰 개수 : ', after)\n",
        "#logit = CLS.inference(sentences=reviews[1000:2000])\n",
        "\n",
        "logit = CLS.inference(sentences=review_list)\n",
        "#logit는 1과 0이 나타나는 리스트\n",
        "#print(logit)  # 네이버 영화평의 경우 0은 부정 카테고리, 1은 긍정 카테고리로 설정되어 있음\n",
        "rate = logit.count(1)/after * 100\n",
        "print(f'긍정 예측 비율 : {rate:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}